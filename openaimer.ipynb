{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11387361,
          "sourceType": "datasetVersion",
          "datasetId": 7130767
        },
        {
          "sourceId": 11440383,
          "sourceType": "datasetVersion",
          "datasetId": 7166561
        },
        {
          "sourceId": 11447798,
          "sourceType": "datasetVersion",
          "datasetId": 7172132
        },
        {
          "sourceId": 11447892,
          "sourceType": "datasetVersion",
          "datasetId": 7172208
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Balancing an Imbalanced Image Dataset via Augmentation\n",
        "\n",
        "**Goal:** Ensure each of 7 classes (`bus`, `car`, `cat`, `dog`, `cricket`, `football`, `product`) ends up with **800** images.\n",
        "\n",
        "---\n",
        "\n",
        "## Concept Overview\n",
        "\n",
        "1. **Resize Originals**  \n",
        "   - Standardize all images to **256 × 256** pixels.\n",
        "\n",
        "2. **Downsample If ≥ 800**  \n",
        "   - Randomly pick 800 originals and resize.\n",
        "\n",
        "3. **Upsample If < 800**  \n",
        "   - Copy and resize all existing images.\n",
        "   - Generate additional ones via simple augmentations:\n",
        "     - **Horizontal flip** (50% chance)  \n",
        "     - **Rotation** (±20°)  \n",
        "     - **Color jitter** (±20% brightness/contrast/saturation)\n",
        "\n",
        "---\n",
        "\n",
        "## Result Snapshot\n",
        "\n",
        "| Class     | Original Count → Final Count |\n",
        "|:----------|:----------------------------:|\n",
        "| bus       | 542 → 800                    |\n",
        "| car       | 469 → 800                    |\n",
        "| cat       | 500 → 800                    |\n",
        "| dog       | 542 → 800                    |\n",
        "| cricket   | 90  →  800                    |\n",
        "| football  | 100 →  800                    |\n",
        "| product   | 800  →  800                    |\n",
        "\n",
        "_All classes are now perfectly balanced for model training._\n"
      ],
      "metadata": {
        "id": "laWCFmYT0Q4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "categories = ['bus', 'car', 'cat', 'dog', 'cricket', 'football', 'product']\n",
        "\n",
        "root_dir = \"/kaggle/input/openaimer-data/OpenAImer2025_Image_Classification/OpenAImer/train\"\n",
        "\n",
        "# Define valid image extensions.\n",
        "valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif')\n",
        "\n",
        "# Define a simple transform for resizing (used for original images).\n",
        "resize_transform = transforms.Resize((256, 256))\n",
        "\n",
        "# Define an augmentation pipeline.\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.Resize((256, 256))\n",
        "])\n",
        "\n",
        "# Function to process and save an image.\n",
        "def process_and_save(img_path, save_path, transform):\n",
        "    try:\n",
        "        with Image.open(img_path) as img:\n",
        "            # Ensure image is in RGB mode (to preserve colour channels)\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            img_processed = transform(img)\n",
        "            img_processed.save(save_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "# Function to get list of image file paths given a directory.\n",
        "def get_image_paths(directory):\n",
        "    return [os.path.join(directory, fname) for fname in os.listdir(directory)\n",
        "            if fname.lower().endswith(valid_extensions)]\n",
        "out_dir = \"/kaggle/working\"\n",
        "# Loop over each category.\n",
        "for cat in categories:\n",
        "    src_dir = os.path.join(root_dir, cat)\n",
        "    dst_dir = os.path.join(out_dir, cat + '-augment')\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "    # Get list of original image paths.\n",
        "    original_paths = get_image_paths(src_dir)\n",
        "    num_originals = len(original_paths)\n",
        "    print(f\"Category '{cat}': {num_originals} original images found.\")\n",
        "\n",
        "    final_count = 800  # Desired count per category\n",
        "\n",
        "    # Prepare list for final images (we will copy or generate until we have exactly final_count images)\n",
        "    saved_count = 0\n",
        "\n",
        "    # If there are too many originals, randomly choose 800 to simply resize and copy.\n",
        "    if num_originals >= final_count:\n",
        "        chosen = random.sample(original_paths, final_count)\n",
        "        for idx, img_path in enumerate(chosen, start=1):\n",
        "            save_name = f\"{cat}_{idx:04d}.jpg\"\n",
        "            save_path = os.path.join(dst_dir, save_name)\n",
        "            process_and_save(img_path, save_path, resize_transform)\n",
        "            saved_count += 1\n",
        "    else:\n",
        "        # First, copy all original images (resized)\n",
        "        for idx, img_path in enumerate(original_paths, start=1):\n",
        "            save_name = f\"{cat}_orig_{idx:04d}.jpg\"\n",
        "            save_path = os.path.join(dst_dir, save_name)\n",
        "            process_and_save(img_path, save_path, resize_transform)\n",
        "            saved_count += 1\n",
        "\n",
        "        # Determine how many augmented images to create.\n",
        "        aug_needed = final_count - saved_count\n",
        "        print(f\"Generating {aug_needed} augmented images for category '{cat}'.\")\n",
        "\n",
        "        for idx in range(aug_needed):\n",
        "            # Randomly choose an image from the originals.\n",
        "            src_img_path = random.choice(original_paths)\n",
        "            # Generate a unique filename.\n",
        "            save_name = f\"{cat}_aug_{saved_count + 1:04d}.jpg\"\n",
        "            save_path = os.path.join(dst_dir, save_name)\n",
        "            process_and_save(src_img_path, save_path, aug_transform)\n",
        "            saved_count += 1\n",
        "\n",
        "    # Verify that exactly 800 images are now in the destination.\n",
        "    final_images = get_image_paths(dst_dir)\n",
        "    print(f\"Finished category '{cat}': {len(final_images)} images in {dst_dir}.\")\n",
        "    if len(final_images) != final_count:\n",
        "        print(f\"Warning: Expected {final_count} images but found {len(final_images)} in {dst_dir}.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T06:02:15.707783Z",
          "iopub.execute_input": "2025-04-18T06:02:15.707984Z",
          "iopub.status.idle": "2025-04-18T06:03:24.306056Z",
          "shell.execute_reply.started": "2025-04-18T06:02:15.707967Z",
          "shell.execute_reply": "2025-04-18T06:03:24.305469Z"
        },
        "id": "s7DRLAbi0Q41",
        "outputId": "7623c084-9efb-4324-c553-7300876ec09b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Category 'bus': 542 original images found.\nGenerating 258 augmented images for category 'bus'.\nFinished category 'bus': 800 images in /kaggle/working/bus-augment.\nCategory 'car': 469 original images found.\nGenerating 331 augmented images for category 'car'.\nFinished category 'car': 800 images in /kaggle/working/car-augment.\nCategory 'cat': 500 original images found.\nGenerating 300 augmented images for category 'cat'.\nFinished category 'cat': 800 images in /kaggle/working/cat-augment.\nCategory 'dog': 542 original images found.\nGenerating 258 augmented images for category 'dog'.\nFinished category 'dog': 800 images in /kaggle/working/dog-augment.\nCategory 'cricket': 90 original images found.\nGenerating 710 augmented images for category 'cricket'.\nFinished category 'cricket': 800 images in /kaggle/working/cricket-augment.\nCategory 'football': 100 original images found.\nGenerating 700 augmented images for category 'football'.\nFinished category 'football': 800 images in /kaggle/working/football-augment.\nCategory 'product': 800 original images found.\nFinished category 'product': 800 images in /kaggle/working/product-augment.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding Generation Through Enhanced Supervised Contrastive Learning**\n",
        "\n",
        "This Python code implements an advanced supervised contrastive learning framework using PyTorch and PyTorch Lightning for image feature extraction. The primary goal is to learn embeddings where same‑class images are **pulled together** and different‑class images are **pushed apart**, with extra emphasis on under‑represented classes and hard negatives.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. **`EnhancedSupConLoss`**  \n",
        "Custom supervised contrastive loss with:\n",
        "- **Temperature scaling** ($\\tau$) and **base temperature** ($\\tau_0$).  \n",
        "- **Class re‑weighting** to up‑weight rare classes:  \n",
        "  $$w_c = \\frac{\\max_{c'} N_{c'}}{N_c},$$  \n",
        "  then scale each anchor’s loss by $w_{y_i}$.  \n",
        "- **Margin** $m$ on negatives:  \n",
        "  $$\\tilde s_{i,a} = \\frac{z_i \\cdot z_a}{\\tau} - m,\\quad a\\in\\mathcal{N}(i).$$  \n",
        "- **Hard negative mining**: select top‑$k$ hardest negatives per anchor,  \n",
        "  $$k = \\max\\bigl(1,\\lfloor r\\cdot|\\mathcal{N}(i)|\\rfloor\\bigr),$$  \n",
        "  where $r$ is the `hard_mining_ratio`.  \n",
        "\n",
        "**Core loss** for anchor $i$ (view 0):  \n",
        "$$\n",
        "\\ell_i = -\\frac{1}{|\\mathcal{P}(i)|}\\sum_{p\\in\\mathcal{P}(i)}\n",
        "\\log\\frac{\\exp\\!\\bigl(z_i\\cdot z_p/\\tau\\bigr)}\n",
        "{\\sum_{a\\in\\mathcal{P}(i)\\cup\\text{hardNeg}(i)}\\exp\\!\\bigl(\\tilde s_{i,a}\\bigr)}\n",
        "\\quad,\\quad\n",
        "L = \\frac{1}{N}\\sum_i w_{y_i}\\,\\ell_i.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **`AdvancedSupConDataset`**  \n",
        "Handles data loading and computes per‑class weights:\n",
        "- **Class weights** for sampler and loss:  \n",
        "  $$w_c = \\frac{\\max_{c'} N_{c'}}{N_c}.$$  \n",
        "- **Strong augmentation pipeline** (for each of $n_{\\mathrm{views}}$):\n",
        "  - Random resized crop, color jitter, grayscale, Gaussian blur  \n",
        "  - Flips, rotation, affine transforms  \n",
        "  - Normalization to ImageNet stats  \n",
        "- Generates **multiple “views”** per image for contrastive pairs.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **`EnhancedEncoder`**  \n",
        "Feature extractor with:\n",
        "- **Pretrained CNN backbone** (ResNet, EfficientNet) We used EfficientNet as it proved to be a better feature extractor than ResNet for our task.  \n",
        "- Optional **attention mechanism**:\n",
        "  $$a = \\sigma(W_2\\,\\mathrm{ReLU}(W_1\\,f))\\,,\\quad f\\in\\mathbb{R}^{d},$$  \n",
        "  then scale features by $a$.  \n",
        "- **Projection head** (MLP) mapping to $d_{\\mathrm{proj}}$, followed by $\\ell_2$ normalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **`AdvancedSupConModule`**  \n",
        "PyTorch Lightning module tying together encoder and loss:\n",
        "- **`forward`** handles multi‑view batching:  \n",
        "  reshape $[B,n_v,C,H,W]\\to[B\\,n_v,C,H,W]\\to[B,n_v,d_{\\mathrm{proj}}]$.  \n",
        "- **`training_step`** computes $L$ via `EnhancedSupConLoss`, logs `train_loss`.  \n",
        "- Optimizer: **AdamW** with **CosineAnnealingWarmRestarts**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **`AdvancedSupConDataModule`**  \n",
        "Lightning data module for train/val split:\n",
        "- Splits dataset by ratio, seeds for reproducibility.\n",
        "- **WeightedRandomSampler** on training set using $\\{w_c\\}$ to ensure balanced batches.\n",
        "- Standard DataLoader for validation (no shuffling).\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **`train_contrastive_model`**  \n",
        "Orchestrator function:\n",
        "- Instantiates `AdvancedSupConDataModule` & `AdvancedSupConModule`.\n",
        "- Configures logging (CSVLogger), callbacks (checkpointing, early stopping, LR monitoring).\n",
        "- Runs `Trainer.fit(...)` on GPU/CPU, saves best & final checkpoints.\n",
        "\n",
        "---\n",
        "\n",
        "By combining **temperature‑scaled contrastive loss**, **class re‑weighting**, **margin‑augmented hard negative mining**, and **strong data augmentations**, this framework yields highly discriminative, class‑balanced embeddings ready for downstream tasks.  \n"
      ],
      "metadata": {
        "id": "MRv_mcN20Q43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, Subset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# --- Enhanced Supervised Contrastive Loss with Class Reweighting and Hard Negative Mining ---\n",
        "class EnhancedSupConLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.05, base_temperature=0.07, contrast_mode='all',\n",
        "                 hard_mining_ratio=0.35, margin=0.2):\n",
        "        super(EnhancedSupConLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.base_temperature = base_temperature\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.hard_mining_ratio = hard_mining_ratio  # Ratio of hard negatives to mine\n",
        "        self.margin = margin  # Margin to push negatives further\n",
        "\n",
        "    def forward(self, features, labels=None, mask=None, class_weights=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: hidden vector of shape [bsz, n_views, ...].\n",
        "            labels: ground truth of shape [bsz].\n",
        "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
        "                has the same class as sample i. Can be asymmetric.\n",
        "            class_weights: dictionary mapping class indices to weights.\n",
        "        Returns:\n",
        "            A loss scalar.\n",
        "        \"\"\"\n",
        "        device = features.device\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "\n",
        "        # Flatten if features is [bsz, n_views, ...]\n",
        "        features = features.view(batch_size, -1, features.shape[-1])\n",
        "\n",
        "        if self.contrast_mode == 'one':\n",
        "            # Use first view as anchor\n",
        "            anchor_feature = features[:, 0]\n",
        "            contrast_feature = features[:, 1]\n",
        "        elif self.contrast_mode == 'all':\n",
        "            # Reshape to (batch_size * n_views, feature_dim)\n",
        "            anchor_feature = features.view(-1, features.shape[-1])\n",
        "            contrast_feature = anchor_feature\n",
        "\n",
        "            if labels is not None:\n",
        "                # Repeat labels for each view\n",
        "                labels = labels.repeat_interleave(features.shape[1])\n",
        "\n",
        "            batch_size = anchor_feature.shape[0]\n",
        "        else:\n",
        "            raise ValueError('Unknown contrast mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "        # Compute logits\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            torch.matmul(anchor_feature, contrast_feature.T),\n",
        "            self.temperature\n",
        "        )\n",
        "\n",
        "        # For numerical stability\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        anchor_dot_contrast = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        # Mask diagonal (self-contrast) and apply margin to negative pairs\n",
        "        logits_mask = 1 - torch.eye(batch_size, device=device)\n",
        "\n",
        "        # Create mask for positive pairs\n",
        "        if mask is None:\n",
        "            mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float().to(device)\n",
        "\n",
        "        # Hard negative mining: find the hardest negatives (highest similarity)\n",
        "        neg_mask = 1 - mask - torch.eye(batch_size, device=device)  # Mask for negative pairs\n",
        "        neg_logits = anchor_dot_contrast * neg_mask\n",
        "\n",
        "        # Apply margin to negative pairs (push them further away)\n",
        "        if self.margin > 0:\n",
        "            neg_logits = neg_logits - self.margin * neg_mask\n",
        "            anchor_dot_contrast = anchor_dot_contrast * mask + neg_logits\n",
        "\n",
        "        # Hard negative mining: select the hardest negatives\n",
        "        if self.hard_mining_ratio < 1.0 and self.hard_mining_ratio > 0:\n",
        "            k = int(batch_size * self.hard_mining_ratio)\n",
        "            k = max(k, 1)  # At least one negative\n",
        "\n",
        "            # For each anchor, find the k hardest negatives\n",
        "            for i in range(batch_size):\n",
        "                neg_logits_i = neg_logits[i]\n",
        "                # Filter zeros (which correspond to positives or self)\n",
        "                valid_indices = torch.where(neg_mask[i] > 0)[0]\n",
        "                if len(valid_indices) > 0:\n",
        "                    valid_logits = neg_logits_i[valid_indices]\n",
        "                    # Take top k if we have enough negatives\n",
        "                    k_actual = min(k, len(valid_indices))\n",
        "                    if k_actual > 0:\n",
        "                        _, hard_indices = torch.topk(valid_logits, k_actual)\n",
        "                        # Create a mask to zero out non-hard negatives\n",
        "                        hard_indices = valid_indices[hard_indices]\n",
        "                        hard_mask = torch.zeros_like(neg_logits_i, device=device)\n",
        "                        hard_mask[hard_indices] = 1.0\n",
        "                        # Update mask to only include hard negatives and positives\n",
        "                        logits_mask[i] = (mask[i] + hard_mask * neg_mask[i]) > 0\n",
        "\n",
        "        # Apply mask to exclude self-contrast cases\n",
        "        final_mask = mask * logits_mask\n",
        "\n",
        "        # Compute log_prob\n",
        "        exp_logits = torch.exp(anchor_dot_contrast) * logits_mask\n",
        "        log_prob = anchor_dot_contrast - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
        "\n",
        "        # Compute mean of log-likelihood over positive\n",
        "        # Weight each class if class_weights is provided\n",
        "        if class_weights is not None and labels is not None:\n",
        "            # Convert class indices to weights\n",
        "            weight_values = torch.tensor([class_weights.get(label.item(), 1.0)\n",
        "                                      for label in labels], device=device)\n",
        "            # Create weight matrix for positive pairs\n",
        "            weight_matrix = weight_values.unsqueeze(1) * final_mask\n",
        "            # Apply weights to log_prob\n",
        "            mean_log_prob_pos = (weight_matrix * log_prob).sum(1) / (weight_matrix.sum(1) + 1e-12)\n",
        "        else:\n",
        "            mean_log_prob_pos = (final_mask * log_prob).sum(1) / (final_mask.sum(1) + 1e-12)\n",
        "\n",
        "        # Loss\n",
        "        loss = -(self.temperature / self.base_temperature) * mean_log_prob_pos\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "# --- Advanced Data Augmentation Strategy ---\n",
        "class AdvancedSupConDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, n_views=2, strong_augment=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.samples = []\n",
        "        self.targets = []  # Store targets separately for sampling\n",
        "\n",
        "        # Collect all samples\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "            for fname in os.listdir(cls_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                    path = os.path.join(cls_dir, fname)\n",
        "                    class_idx = self.class_to_idx[cls]\n",
        "                    self.samples.append((path, class_idx))\n",
        "                    self.targets.append(class_idx)\n",
        "\n",
        "        # Count class occurrences for class distribution\n",
        "        self.class_counts = Counter(self.targets)\n",
        "        print(f\"Class distribution: {self.class_counts}\")\n",
        "\n",
        "        # Calculate class weights (inverse of frequency)\n",
        "        max_count = max(self.class_counts.values())\n",
        "        self.class_weights = {cls: max_count / count for cls, count in self.class_counts.items()}\n",
        "\n",
        "        # Base transform\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((256, 256)),  # Larger size for better detail\n",
        "            transforms.CenterCrop(224),     # Standard size for ResNet\n",
        "        ])\n",
        "\n",
        "        # ImageNet normalization\n",
        "        imagenet_norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                            std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        # Strong augmentation strategy for contrastive learning\n",
        "        if strong_augment:\n",
        "            augmentation_pipeline = [\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.2, 1.0)),\n",
        "                transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "                transforms.ToTensor(),\n",
        "                imagenet_norm\n",
        "            ]\n",
        "        else:\n",
        "            # Simpler augmentation for less distortion\n",
        "            augmentation_pipeline = [\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "                transforms.ToTensor(),\n",
        "                imagenet_norm\n",
        "            ]\n",
        "\n",
        "        self.augmentations = transforms.Compose(augmentation_pipeline)\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, label = self.samples[index]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        # Apply base transform\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Generate n_views different augmented versions\n",
        "        views = torch.stack([self.augmentations(image) for _ in range(self.n_views)], dim=0)\n",
        "\n",
        "        return views, label\n",
        "\n",
        "# --- Enhanced Feature Extractor Network ---\n",
        "class EnhancedEncoder(nn.Module):\n",
        "    def __init__(self, backbone='resnet50', proj_dim=512, dropout=0.3, use_attention=True):\n",
        "        super(EnhancedEncoder, self).__init__()\n",
        "\n",
        "        # Load pretrained backbone\n",
        "        if backbone == 'resnet50':\n",
        "            base_model = models.resnet50(pretrained=True)\n",
        "            feat_dim = 2048\n",
        "        elif backbone == 'resnet101':\n",
        "            base_model = models.resnet101(pretrained=True)\n",
        "            feat_dim = 2048\n",
        "        elif backbone == 'efficientnet_b3':\n",
        "            base_model = models.efficientnet_b3(pretrained=True)\n",
        "            feat_dim = 1536\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # Remove classification head\n",
        "        self.encoder = nn.Sequential(*list(base_model.children())[:-1])\n",
        "\n",
        "        # Attention mechanism (optional)\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attention = nn.Sequential(\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(feat_dim, feat_dim // 8),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(feat_dim // 8, feat_dim),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        # Non-linear projection head (MLP)\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(feat_dim, feat_dim),\n",
        "            nn.BatchNorm1d(feat_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feat_dim, feat_dim // 2),\n",
        "            nn.BatchNorm1d(feat_dim // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feat_dim // 2, proj_dim),\n",
        "            nn.BatchNorm1d(proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "\n",
        "        if self.use_attention:\n",
        "            # Apply attention mechanism\n",
        "            flat_features = features.view(features.size(0), -1)\n",
        "            attention_weights = self.attention(features)\n",
        "            weighted_features = flat_features * attention_weights\n",
        "            weighted_features = weighted_features.view_as(features)\n",
        "            features = weighted_features\n",
        "\n",
        "        # Project features\n",
        "        projections = self.projector(features)\n",
        "\n",
        "        # L2 normalize embeddings\n",
        "        projections = F.normalize(projections, dim=1)\n",
        "\n",
        "        return projections\n",
        "\n",
        "# --- Lightning Module for Advanced SupCon ---\n",
        "class AdvancedSupConModule(pl.LightningModule):\n",
        "    def __init__(self, encoder_type='resnet50', proj_dim=512, learning_rate=3e-4,\n",
        "                 weight_decay=1e-4, temperature=0.07, use_attention=True,\n",
        "                 hard_mining_ratio=0.5, margin=0.1):\n",
        "        super(AdvancedSupConModule, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Initialize enhanced encoder\n",
        "        self.encoder = EnhancedEncoder(\n",
        "            backbone=encoder_type,\n",
        "            proj_dim=proj_dim,\n",
        "            dropout=0.3,\n",
        "            use_attention=use_attention\n",
        "        )\n",
        "\n",
        "        # Loss function with class weights passed at training time\n",
        "        self.criterion = EnhancedSupConLoss(\n",
        "            temperature=temperature,\n",
        "            hard_mining_ratio=hard_mining_ratio,\n",
        "            margin=margin\n",
        "        )\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.dataset_class_weights = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # For inference/validation reshape (only one view at test time)\n",
        "        if len(x.shape) == 4:  # [B, C, H, W]\n",
        "            return self.encoder(x)\n",
        "\n",
        "        # For training with multiple views [B, n_views, C, H, W]\n",
        "        batch_size, n_views, C, H, W = x.shape\n",
        "        x = x.view(-1, C, H, W)  # Reshape to [B*n_views, C, H, W]\n",
        "        features = self.encoder(x)  # Get embeddings [B*n_views, proj_dim]\n",
        "        features = features.view(batch_size, n_views, -1)  # Reshape to [B, n_views, proj_dim]\n",
        "        return features\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        views, labels = batch  # views shape: [B, n_views, C, H, W]\n",
        "        features = self(views)\n",
        "\n",
        "        # Calculate loss with class weights if available\n",
        "        loss = self.criterion(features, labels, class_weights=self.dataset_class_weights)\n",
        "\n",
        "        # Log metrics\n",
        "        self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.train_losses.append(loss.detach())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        views, labels = batch\n",
        "        if isinstance(views, list):\n",
        "            # Handle different formats\n",
        "            first_view = views[0]\n",
        "        else:\n",
        "            # Take only first view for validation\n",
        "            first_view = views[:, 0]\n",
        "\n",
        "        embeddings = self.encoder(first_view)\n",
        "\n",
        "        # Just log that validation was performed\n",
        "        self.log('val_step', batch_idx)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        if len(self.train_losses) > 0:\n",
        "            avg_loss = torch.stack(self.train_losses).mean()\n",
        "            self.log('epoch_loss', avg_loss)\n",
        "            print(f\"Epoch {self.current_epoch} --> Average Loss: {avg_loss:.4f}\")\n",
        "            self.train_losses.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Use AdamW optimizer\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams.learning_rate,\n",
        "            weight_decay=self.hparams.weight_decay\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=10,\n",
        "            T_mult=2,\n",
        "            eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",\n",
        "                \"monitor\": \"train_loss\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "# --- Advanced Data Module ---\n",
        "class AdvancedSupConDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=32, num_workers=4, strong_augment=True, val_split=0.1):\n",
        "        super(AdvancedSupConDataModule, self).__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.strong_augment = strong_augment\n",
        "        self.val_split = val_split\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Create full dataset\n",
        "        self.full_dataset = AdvancedSupConDataset(\n",
        "            root_dir=self.data_dir,\n",
        "            strong_augment=self.strong_augment\n",
        "        )\n",
        "\n",
        "        # Get dataset size and calculate split indices\n",
        "        dataset_size = len(self.full_dataset)\n",
        "        val_size = int(self.val_split * dataset_size)\n",
        "        train_size = dataset_size - val_size\n",
        "\n",
        "        # Create train/val splits\n",
        "        indices = list(range(dataset_size))\n",
        "        # Randomize the indices\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_indices = indices[:train_size]\n",
        "        val_indices = indices[train_size:]\n",
        "\n",
        "        # Create subsets\n",
        "        self.train_dataset = Subset(self.full_dataset, train_indices)\n",
        "        self.val_dataset = Subset(self.full_dataset, val_indices)\n",
        "\n",
        "        # We need to keep track of targets for the sampler\n",
        "        self.train_targets = [self.full_dataset.targets[i] for i in train_indices]\n",
        "\n",
        "        # Store class weights\n",
        "        self.class_weights = self.full_dataset.class_weights\n",
        "\n",
        "        print(f\"Train size: {train_size}, Validation size: {val_size}\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Create a weighted sampler for the training set\n",
        "        class_counts = Counter(self.train_targets)\n",
        "        max_samples = max(class_counts.values())\n",
        "        weights = [self.class_weights[target] for target in self.train_targets]\n",
        "\n",
        "        sampler = WeightedRandomSampler(\n",
        "            weights=weights,\n",
        "            num_samples=len(self.train_targets),\n",
        "            replacement=True\n",
        "        )\n",
        "\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            sampler=sampler,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "# --- Main Training Function ---\n",
        "def train_contrastive_model(train_dir, batch_size=16, max_epochs=400,\n",
        "                          encoder_type='resnet50', proj_dim=512,\n",
        "                          learning_rate=3e-4, use_attention=True):\n",
        "\n",
        "    # Initialize data module\n",
        "    data_module = AdvancedSupConDataModule(\n",
        "        data_dir=train_dir,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4,\n",
        "        strong_augment=True,\n",
        "        val_split=0.1\n",
        "    )\n",
        "\n",
        "    # Set up data module\n",
        "    data_module.setup()\n",
        "\n",
        "    # Initialize model\n",
        "    model = AdvancedSupConModule(\n",
        "        encoder_type=encoder_type,\n",
        "        proj_dim=proj_dim,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=1e-4,\n",
        "        temperature=0.07,\n",
        "        use_attention=use_attention,\n",
        "        hard_mining_ratio=0.5,\n",
        "        margin=0.1\n",
        "    )\n",
        "\n",
        "    # Pass dataset class weights to model\n",
        "    model.dataset_class_weights = data_module.class_weights\n",
        "\n",
        "    # Set up logging\n",
        "    csv_logger = CSVLogger(\"logs/\", name=\"advanced_supcon_run\")\n",
        "\n",
        "    # Set up callbacks\n",
        "    checkpoint_callback_best = ModelCheckpoint(\n",
        "        monitor='train_loss',\n",
        "        mode='min',\n",
        "        save_top_k=1,\n",
        "        filename='best-{epoch:02d}-{train_loss:.4f}-efficientnet_b3'\n",
        "    )\n",
        "\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='train_loss',\n",
        "        patience=30,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "    # Set up trainer\n",
        "    if torch.cuda.is_available():\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=20,\n",
        "            accelerator='gpu',\n",
        "            devices=1,\n",
        "            logger=csv_logger,\n",
        "            callbacks=[checkpoint_callback_best, early_stopping, lr_monitor],\n",
        "            gradient_clip_val=1.0,  # Clip gradients to avoid exploding gradients\n",
        "        )\n",
        "    else:\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=max_epochs,\n",
        "            accelerator='cpu',\n",
        "            devices=1,\n",
        "            logger=csv_logger,\n",
        "            callbacks=[checkpoint_callback_best,early_stopping, lr_monitor],\n",
        "            gradient_clip_val=1.0,\n",
        "        )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "\n",
        "    # Save final model\n",
        "    trainer.save_checkpoint(\"advanced_supcon_final_resnet101.ckpt\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == '__main__':\n",
        "    train_dir = '/kaggle/input/augmented-256/augmented_dataset'\n",
        "    model = train_contrastive_model(\n",
        "        train_dir=train_dir,\n",
        "        batch_size=16,  # Reduced batch size to avoid CUDA OOM\n",
        "        max_epochs=20,\n",
        "        encoder_type='efficientnet_b3',\n",
        "        proj_dim=1024,\n",
        "        learning_rate=3e-4,\n",
        "        use_attention=True\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T06:06:29.184018Z",
          "iopub.execute_input": "2025-04-18T06:06:29.184816Z",
          "iopub.status.idle": "2025-04-18T06:59:49.085123Z",
          "shell.execute_reply.started": "2025-04-18T06:06:29.184789Z",
          "shell.execute_reply": "2025-04-18T06:59:49.084235Z"
        },
        "colab": {
          "referenced_widgets": [
            "",
            "0afc8ac56599484ba4756edd3d2bfa19"
          ]
        },
        "id": "q9gC0QZ90Q44",
        "outputId": "cdb157fe-b583-446c-feca-469f50d74bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Class distribution: Counter({0: 800, 1: 800, 2: 800, 3: 800, 4: 800, 5: 800, 6: 800})\nTrain size: 5040, Validation size: 560\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n100%|██████████| 47.2M/47.2M [00:00<00:00, 181MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Class distribution: Counter({0: 800, 1: 800, 2: 800, 3: 800, 4: 800, 5: 800, 6: 800})\nTrain size: 5040, Validation size: 560\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0afc8ac56599484ba4756edd3d2bfa19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 0 --> Average Loss: 3.6431\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 1 --> Average Loss: 2.5069\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 2 --> Average Loss: 2.2393\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 3 --> Average Loss: 2.1468\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 4 --> Average Loss: 2.0106\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 5 --> Average Loss: 2.0187\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 6 --> Average Loss: 1.8905\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 7 --> Average Loss: 1.8807\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 8 --> Average Loss: 1.8496\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 9 --> Average Loss: 1.8547\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 10 --> Average Loss: 1.9926\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 11 --> Average Loss: 2.0534\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 12 --> Average Loss: 1.9950\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 13 --> Average Loss: 1.9334\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 14 --> Average Loss: 1.9680\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 15 --> Average Loss: 1.9157\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 16 --> Average Loss: 1.8918\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 17 --> Average Loss: 1.8897\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 18 --> Average Loss: 1.8563\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 19 --> Average Loss: 1.8472\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Classification Using MLP and Focal Loss - Key Mathematical Concepts\n",
        "\n",
        "This code implements an image classification pipeline using PyTorch Lightning that leverages features from a pre-trained Supervised Contrastive (SupCon) model. The approach employs transfer learning: utilizing powerful representations learned during SupCon pre-training for downstream classification tasks.\n",
        "\n",
        "## Key Mathematical Components\n",
        "\n",
        "### 1. Focal Loss for Class Imbalance\n",
        "The `FocalLoss` class implements a specialized loss function designed to address class imbalance:\n",
        "\n",
        "- **Mathematical formulation**:\n",
        "  ```\n",
        "  FL(pt) = -αt * (1-pt)^γ * log(pt)\n",
        "  ```\n",
        "  - Where pt is the model's probability estimate for the true class\n",
        "  - γ (gamma) is the focusing parameter (set to 2.0 by default)\n",
        "  - αt are class weights inversely proportional to frequencies\n",
        "\n",
        "- The focusing parameter down-weights easy examples, forcing the model to concentrate on difficult cases\n",
        "- Class-specific weights compensate for imbalanced distributions in the dataset\n",
        "\n",
        "### 2. Transfer Learning Architecture\n",
        "\n",
        "The `LinearClassifierFromSupCon` utilizes a frozen pre-trained model with a trainable MLP:\n",
        "\n",
        "- **Feature extraction**: Pre-trained SupCon model maps images to embeddings\n",
        "  - These embeddings capture semantic information learned during contrastive training\n",
        "  - Weights are frozen to preserve learned representations\n",
        "\n",
        "- **MLP classifier head**:\n",
        "  ```\n",
        "  Input (proj_dim) → Linear → BN → ReLU → Dropout →\n",
        "  Linear → BN → ReLU → Dropout → Linear → Output (num_classes)\n",
        "  ```\n",
        "  - Dimensions: proj_dim → 256 → 128 → num_classes\n",
        "  - BatchNorm layers normalize activations, stabilizing training\n",
        "  - Dropout (0.3) provides regularization to prevent overfitting\n",
        "\n",
        "### 3. Evaluation Metrics\n",
        "\n",
        "- **Macro F1-Score**: Primary performance metric\n",
        "  ```\n",
        "  F1 = 2 * (precision * recall) / (precision + recall)\n",
        "  ```\n",
        "  - Calculated per-class then averaged evenly across all classes\n",
        "  - Appropriate for imbalanced datasets as it gives equal importance to minority classes\n",
        "\n",
        "### 4. Data Handling and Training\n",
        "\n",
        "- **Stratified sampling**: Maintains class distribution between training and validation sets\n",
        "- **Data normalization**: Images normalized using ImageNet statistics\n",
        "  ```\n",
        "  mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "  ```\n",
        "- **Optimization**: AdamW optimizer combines Adam with decoupled weight decay\n",
        "- **Training monitoring**: Early stopping based on validation F1-score prevents overfitting\n",
        "\n",
        "## Implementation Structure\n",
        "\n",
        "The code maintains a clean separation between:\n",
        "- Data preparation (`ClassificationDataset`, `ClassifierDataModule`)\n",
        "- Model architecture (`LinearClassifierFromSupCon`)\n",
        "- Training logic (PyTorch Lightning Trainer)\n",
        "\n",
        "This modular design allows for easy experimentation with different models, datasets, and hyperparameters while maintaining the core mathematical principles behind the classification approach."
      ],
      "metadata": {
        "id": "vQNUxdNM0Q45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from torchmetrics.classification import MulticlassF1Score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Focal Loss for class imbalance ---\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=1.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if alpha is None:\n",
        "\n",
        "            frequencies = torch.tensor([800,800,800,800,800,800,800],dtype=torch.float)\n",
        "            alpha = 1.0 / frequencies\n",
        "            alpha = alpha / alpha.sum()\n",
        "        self.register_buffer('alpha', alpha)\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # inputs: logits [N, C], targets: [N]\n",
        "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
        "        probs = torch.softmax(inputs, dim=1)\n",
        "        one_hot = nn.functional.one_hot(targets, num_classes=inputs.size(1)).float()\n",
        "        p_t = (probs * one_hot).sum(dim=1)\n",
        "        alpha_t = self.alpha[targets]\n",
        "        focal = (1 - p_t) ** self.gamma\n",
        "        loss = alpha_t * focal * ce_loss\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        if self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss\n",
        "\n",
        "# --- Image Classification Dataset ---\n",
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.classes = sorted([\n",
        "            d for d in os.listdir(root_dir)\n",
        "            if os.path.isdir(os.path.join(root_dir, d))\n",
        "        ])\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "        self.samples = []\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "            for fname in os.listdir(cls_dir):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                    path = os.path.join(cls_dir, fname)\n",
        "                    self.samples.append((path, self.class_to_idx[cls]))\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# --- DataModule for classification ---\n",
        "class ClassifierDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=32, num_workers=4, train_split=0.8):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.train_split = train_split\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        full = ClassificationDataset(self.data_dir, transform=self.transform)\n",
        "        targets = [full[i][1] for i in range(len(full))]\n",
        "        idx = list(range(len(full)))\n",
        "        train_idx, val_idx = train_test_split(\n",
        "            idx, train_size=self.train_split,\n",
        "            stratify=targets, random_state=42\n",
        "        )\n",
        "        self.train_ds = Subset(full, train_idx)\n",
        "        self.val_ds   = Subset(full, val_idx)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_ds, batch_size=self.batch_size,\n",
        "            shuffle=True, num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds, batch_size=self.batch_size,\n",
        "            shuffle=False, num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_ds, batch_size=self.batch_size,\n",
        "            shuffle=False, num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "# --- Classifier using frozen SupCon model ---\n",
        "class LinearClassifierFromSupCon(pl.LightningModule):\n",
        "    def __init__(self, supcon_model: pl.LightningModule, num_classes=7,\n",
        "                 learning_rate=1e-3, focal_gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['supcon_model'])\n",
        "        # Freeze SupCon\n",
        "        self.supcon_model = supcon_model\n",
        "        self.supcon_model.eval()\n",
        "        for p in self.supcon_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.supcon_model.hparams.proj_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "        self.criterion = FocalLoss(gamma=focal_gamma)\n",
        "        self.f1 = MulticlassF1Score(num_classes=num_classes, average='macro')\n",
        "        self.train_losses = []\n",
        "    def forward(self, x):\n",
        "        # x: [B,C,H,W]\n",
        "        # get SupCon embedding\n",
        "        with torch.no_grad():\n",
        "            emb = self.supcon_model(x)\n",
        "        # emb: [B, proj_dim]\n",
        "        logits = self.classifier(emb)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        logits = self(imgs)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.train_losses.append(loss.detach())  # Store the detached loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        logits = self(imgs)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        self.f1.update(preds, labels)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        if self.train_losses:  # Ensure there are losses to average\n",
        "            avg_loss = torch.stack(self.train_losses).mean()\n",
        "            self.log('train_loss_epoch', avg_loss, on_epoch=True, prog_bar=True)\n",
        "            print(f\"Epoch {self.current_epoch} --> Train Loss: {avg_loss:.4f}\")\n",
        "            self.train_losses.clear()  # Reset for next epoch\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        f1 = self.f1.compute()\n",
        "        self.print(f\"Epoch {self.current_epoch} --> {f1:.4f}\")\n",
        "        self.log('val_f1', f1, prog_bar=True)\n",
        "        self.f1.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        logits = self(imgs)\n",
        "        loss = self.criterion(logits, labels)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        self.f1.update(preds, labels)\n",
        "        self.log('test_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        f1 = self.f1.compute()\n",
        "        self.log('test_f1', f1, prog_bar=True)\n",
        "        self.f1.reset()\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.classifier.parameters(), lr=self.hparams.learning_rate)\n",
        "        return opt\n",
        "\n",
        "# --- Entrypoint ---\n",
        "\n",
        "def main_classification(checkpoint_path, data_dir):\n",
        "    # load SupCon checkpoint\n",
        "    supcon =AdvancedSupConModule.load_from_checkpoint(checkpoint_path)\n",
        "    # data\n",
        "    dm = ClassifierDataModule(data_dir)\n",
        "    # classifier\n",
        "    clf = LinearClassifierFromSupCon(supcon_model=supcon)\n",
        "\n",
        "    csv_logger = CSVLogger(save_dir=\"logs/\", name=\"classifier_run\")\n",
        "\n",
        "    # trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=50,\n",
        "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "        logger=csv_logger,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(monitor='val_f1', mode='max', save_last=True),\n",
        "            EarlyStopping(monitor='val_f1', mode='max', patience=10)\n",
        "        ]\n",
        "    )\n",
        "    trainer.fit(clf, dm)\n",
        "    trainer.test(clf, dm)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main_classification(\n",
        "        checkpoint_path='/kaggle/input/advanced-supcon-final-resnet101-2/advanced_supcon_final_resnet101 (2).ckpt',\n",
        "        data_dir='/kaggle/input/augmented-256/augmented_dataset'\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T07:08:47.103238Z",
          "iopub.execute_input": "2025-04-18T07:08:47.104100Z",
          "iopub.status.idle": "2025-04-18T07:11:26.717940Z",
          "shell.execute_reply.started": "2025-04-18T07:08:47.104069Z",
          "shell.execute_reply": "2025-04-18T07:11:26.717247Z"
        },
        "colab": {
          "referenced_widgets": [
            "",
            "c46ca531d5034ecfb8ff9631ab4f7a92",
            "df6039b4c88f4dabb503c7f15131ef8b"
          ]
        },
        "id": "twsXrWXN0Q46",
        "outputId": "59af9a1c-5057-4631-dae1-74e38cd8f5ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 0 --> 0.0419\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c46ca531d5034ecfb8ff9631ab4f7a92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 0 --> 1.0000\nEpoch 0 --> Train Loss: 0.0091\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 1 --> 1.0000\nEpoch 1 --> Train Loss: 0.0015\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 2 --> 1.0000\nEpoch 2 --> Train Loss: 0.0006\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 3 --> 1.0000\nEpoch 3 --> Train Loss: 0.0009\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 4 --> 1.0000\nEpoch 4 --> Train Loss: 0.0012\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 5 --> 1.0000\nEpoch 5 --> Train Loss: 0.0007\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 6 --> 1.0000\nEpoch 6 --> Train Loss: 0.0010\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 7 --> 1.0000\nEpoch 7 --> Train Loss: 0.0005\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 8 --> 1.0000\nEpoch 8 --> Train Loss: 0.0006\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 9 --> 1.0000\nEpoch 9 --> Train Loss: 0.0006\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 10 --> 0.9991\nEpoch 10 --> Train Loss: 0.0008\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df6039b4c88f4dabb503c7f15131ef8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9991071820259094    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  7.872776768635958e-05  \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9991071820259094     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   7.872776768635958e-05   </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Pipeline for Embedding Classification\n",
        "\n",
        "This code implements an inference pipeline for the previously defined embedding classification model. The pipeline handles loading test images, running them through the trained classifier, and generating predictions in a CSV format suitable for submission.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "### 1. Test Dataset Definition\n",
        "\n",
        "The `TestDatasetForClassifier` class:\n",
        "- Loads images from a specified directory\n",
        "- Extracts image IDs from filenames using regex\n",
        "- Applies consistent preprocessing transformations:\n",
        "  ```python\n",
        "  transforms.Compose([\n",
        "      transforms.Resize((128, 128)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  ```\n",
        "- Returns tuples of (transformed_image, image_id) for inference\n",
        "\n",
        "### 2. DataLoader Creation\n",
        "\n",
        "The `get_test_dataloader` function:\n",
        "- Creates a PyTorch DataLoader for efficient batch processing\n",
        "- Uses the `TestDatasetForClassifier` with specified parameters\n",
        "- Returns a DataLoader with configurable batch size and worker count\n",
        "\n",
        "### 3. Inference Logic\n",
        "\n",
        "The `run_inference_linear_classifier` function:\n",
        "- Sets the model to evaluation mode\n",
        "- Processes images in batches for efficiency\n",
        "- Performs forward pass through the model to get logits\n",
        "- Converts prediction indices to human-readable labels\n",
        "- Returns list of (image_id, predicted_label) pairs\n",
        "\n",
        "### 4. Main Execution Flow\n",
        "\n",
        "The `main` function orchestrates the entire process:\n",
        "1. Defines paths to test data and model checkpoints\n",
        "2. Loads the trained classifier model from checkpoint\n",
        "3. Creates the test DataLoader\n",
        "4. Runs inference on all test images\n",
        "5. Saves predictions to a CSV file in the required format\n",
        "\n",
        "## Usage\n",
        "\n",
        "The pipeline uses a pre-defined set of class labels:\n",
        "```python\n",
        "LABELS = ['bus', 'car', 'cat', 'cricket', 'dog', 'football', 'product']\n",
        "```\n",
        "\n",
        "This inference pipeline complements the training pipeline by providing a streamlined way to generate predictions from the trained model on new, unseen data."
      ],
      "metadata": {
        "id": "2BVVBS3r0Q46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "# List of label names in order (same as in your classifier)\n",
        "LABELS = ['bus', 'car', 'cat', 'cricket', 'dog', 'football', 'product']\n",
        "\n",
        "# ------------------------\n",
        "# 1. Test Dataset Definition\n",
        "# ------------------------\n",
        "class TestDatasetForClassifier(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Directory containing test images.\n",
        "            transform (callable, optional): Image transform to apply.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        # List only image files\n",
        "        self.filenames = [f for f in os.listdir(root_dir)\n",
        "                          if os.path.isfile(os.path.join(root_dir, f))\n",
        "                          and f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def _extract_id(self, filename):\n",
        "        # This regex extracts the trailing number from the filename.\n",
        "        match = re.search(r'(\\d+)(?:\\.\\w+)?$', filename)\n",
        "        return match.group(1) if match else filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.filenames[idx]\n",
        "        img_path = os.path.join(self.root_dir, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image_id = self._extract_id(filename)\n",
        "        return image, image_id\n",
        "\n",
        "# ------------------------\n",
        "# 2. Create Test DataLoader\n",
        "# ------------------------\n",
        "def get_test_dataloader(test_dir, batch_size=32, num_workers=4):\n",
        "    dataset = TestDatasetForClassifier(root_dir=test_dir)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Inference Function\n",
        "# ------------------------\n",
        "def run_inference_linear_classifier(model, test_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    results = []  # will hold tuples: (id, predicted_label)\n",
        "    with torch.no_grad():\n",
        "        for images, ids in test_loader:\n",
        "            images = images.to(device)\n",
        "            # Forward pass: classifier accepts images of shape [B, C, H, W]\n",
        "            logits = model(images)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            preds = preds.cpu().numpy()\n",
        "            for image_id, pred_idx in zip(ids, preds):\n",
        "                label = LABELS[pred_idx]\n",
        "                results.append((image_id, label))\n",
        "    return results\n",
        "\n",
        "# ------------------------\n",
        "# 4. Main Inference Routine\n",
        "# ------------------------\n",
        "def main():\n",
        "\n",
        "    test_dir = '/kaggle/input/openaimer-data/OpenAImer2025_Image_Classification/OpenAImer/test'  # test image folder path\n",
        "    classifier_checkpoint = '/kaggle/input/new-classifier-3/epoch10_classifier(3).ckpt'  # update with your checkpoint path\n",
        "\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "    model = LinearClassifierFromSupCon.load_from_checkpoint(classifier_checkpoint,supcon_model=AdvancedSupConModule.load_from_checkpoint('/kaggle/input/advanced-supcon-final-resnet101-2/advanced_supcon_final_resnet101 (2).ckpt'))\n",
        "\n",
        "\n",
        "    model.freeze()\n",
        "\n",
        "\n",
        "    test_loader = get_test_dataloader(test_dir, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "\n",
        "    predictions = run_inference_linear_classifier(model, test_loader, device)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(predictions, columns=['id', 'label'])\n",
        "    output_csv_path = '/kaggle/working/test_predictions.csv'\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")\n",
        "      # Display a preview of the predictions\n",
        "    print(\"\\nPreview of predictions:\")\n",
        "    print(df.head(10))  # Show first 10 rows\n",
        "\n",
        "    #  Show class distribution\n",
        "    print(\"\\nClass distribution in predictions:\")\n",
        "    print(df['label'].value_counts())\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T07:16:11.549148Z",
          "iopub.execute_input": "2025-04-18T07:16:11.549476Z",
          "iopub.status.idle": "2025-04-18T07:16:25.040780Z",
          "shell.execute_reply.started": "2025-04-18T07:16:11.549449Z",
          "shell.execute_reply": "2025-04-18T07:16:25.039841Z"
        },
        "id": "UcS058a_0Q47",
        "outputId": "1a3224b0-c81d-4493-c70f-df763d10bd12"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Predictions saved to /kaggle/working/test_predictions.csv\n\nPreview of predictions:\n       id    label\n0   63392      cat\n1   60822      cat\n2  287801      bus\n3  385827  product\n4  175511      dog\n5  249279  product\n6  225936      bus\n7  450136  product\n8  410832      dog\n9  371055      car\n\nClass distribution in predictions:\nlabel\nproduct     505\nbus         268\ndog         263\ncar         141\ncat         139\nfootball     59\ncricket      26\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model checkpoints and augmented datasets are accessible at the specified paths:\n",
        "* Since we already trained our model,we use our checkpoints using which our submissions were made\n",
        "- Classifier checkpoint: \"https://www.kaggle.com/datasets/alcidesthegreat/new-classifier-3/data\"\n",
        "- SupCon model checkpoint: \"https://www.kaggle.com/datasets/alcidesthegreat/advanced-supcon-final-resnet101-2/data\"\n",
        "- Augmented dataset: \"https://www.kaggle.com/datasets/alcidesthegreat/augmented-256\"\n",
        "- Original data: \"https://www.kaggle.com/datasets/anubhabbhattacharya7/openaimer-data\"\n",
        "\n"
      ],
      "metadata": {
        "id": "_YVTQJSq0Q47"
      }
    }
  ]
}